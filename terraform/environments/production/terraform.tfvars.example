# Example Terraform variables file
# Copy this to terraform.tfvars and update with your values

cluster_name = "ai-mlops-production"
kubernetes_version = "1.28"
aws_region = "us-west-2"

# VPC Configuration
vpc_id = "vpc-xxxxxxxxxxxxxxxxx"
subnet_ids = [
  "subnet-xxxxxxxxxxxxxxxxx",
  "subnet-yyyyyyyyyyyyyyyyy",
  "subnet-zzzzzzzzzzzzzzzzz"
]

# Cluster Access
public_endpoint = false
public_access_cidrs = [
  "10.0.0.0/8"  # Your corporate network
]
allowed_cidr_blocks = [
  "10.0.0.0/8"
]

# Logging
enabled_cluster_log_types = [
  "api",
  "audit",
  "authenticator",
  "controllerManager",
  "scheduler"
]
log_retention_days = 30

# CPU Node Pool
cpu_instance_type = "m5.4xlarge"
cpu_desired_size = 2
cpu_max_size = 10
cpu_min_size = 1
cpu_disk_size = 100

# A100 Time-Slicing Node Pool
a100_instance_type = "p5.48xlarge"  # 8x A100 80GB
a100_timeslice_desired_size = 1
a100_timeslice_max_size = 5
a100_timeslice_min_size = 0

# A100 MIG Node Pool
a100_mig_desired_size = 1
a100_mig_max_size = 3
a100_mig_min_size = 1

# H100 Time-Slicing Node Pool
h100_instance_type = "p5.48xlarge"  # 8x H100 80GB
h100_timeslice_desired_size = 1
h100_timeslice_max_size = 5
h100_timeslice_min_size = 0

# GPU Disk Size
gpu_disk_size = 500

# Storage
efs_performance_mode = "generalPurpose"
efs_throughput_mode = "bursting"
additional_efs_cidr_blocks = []

# S3 Buckets for model artifacts and datasets
s3_bucket_arns = [
  "arn:aws:s3:::mlflow-artifacts",
  "arn:aws:s3:::model-registry",
  "arn:aws:s3:::training-datasets"
]

# Tags
tags = {
  Environment = "production"
  Project     = "ai-mlops"
  ManagedBy   = "terraform"
  Team        = "ml-platform"
}

